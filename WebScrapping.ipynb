{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d69c00-696f-4087-955f-fab5606aa36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a4593-e151-4fcd-9a1a-b2866080f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "<table class=\"table\">\n",
    "\n",
    "<tbody><tr>\n",
    "<td>BrickLink</td>\n",
    "<td>\n",
    "<a href=\"https://www.bricklink.com/v2/catalog/catalogitem.page?P=18674&amp;utm_source=rebrickable#T=S\" target=\"_blank\">\n",
    "18674\n",
    "</a>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BrickOwl</td>\n",
    "<td>\n",
    "<a href=\"https://brickowl.com/catalog/627945?utm_source=rebrickable\" target=\"_blank\">\n",
    "627945\n",
    "</a>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Brickset</td>\n",
    "<td>\n",
    "<a href=\"https://brickset.com/parts/design-18674\" target=\"_blank\">\n",
    "18674\n",
    "</a>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LDraw</td>\n",
    "<td>\n",
    "<a href=\"https://library.ldraw.org/search/part?s=18674\" target=\"_blank\">\n",
    "18674\n",
    "</a>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LEGO</td>\n",
    "<td>\n",
    "18674\n",
    "</td>\n",
    "</tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8149807b-d395-49ea-9a3d-96fddbfd45d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rebrickable</th>\n",
       "      <th>BrickLink</th>\n",
       "      <th>BrickOwl</th>\n",
       "      <th>Brickset</th>\n",
       "      <th>LDraw</th>\n",
       "      <th>LEGO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12357</td>\n",
       "      <td>6180pb164</td>\n",
       "      <td>536153</td>\n",
       "      <td>102786</td>\n",
       "      <td>6180pr0031</td>\n",
       "      <td>102786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rebrickable  BrickLink BrickOwl Brickset       LDraw    LEGO\n",
       "0       12357  6180pb164   536153   102786  6180pr0031  102786"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL to scrape\n",
    "url = 'https://rebrickable.com/parts/6180pr0031/plate-special-4-x-6-with-studs-on-3-edges-with-lego-star-wars-captain-rex-print/'\n",
    "df = pd.DataFrame()\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table= soup.find_all('table')[2]\n",
    "    my_dict={'Rebrickable':'12357'}\n",
    "    for row in table.find_all('tr'): \n",
    "        columns = row.find_all('td')\n",
    "        if(columns != []):\n",
    "            site = columns[0].text.strip()\n",
    "            code = columns[1].text.strip()\n",
    "            my_dict[site] = code\n",
    "    df = pd.concat([df, pd.DataFrame([my_dict])], ignore_index=True)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fc898-fd97-4201-852f-2d11ad5dd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    articles = soup.find_all('table', class_='table')\n",
    "    df = pd.DataFrame(columns=['Site', 'Code'])\n",
    "    for row in articles.tbody.find_all('tr'):    \n",
    "        # Find all data for each column\n",
    "        columns = row.find_all('td')\n",
    "    \n",
    "        if(columns != []):\n",
    "            site = columns[0].text.strip()\n",
    "            code = columns[1].text.strip()\n",
    "            df = df.append({'Site': site,  'Code': code}, ignore_index=True)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41b67c-18e0-4eb8-842c-3c0347661a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    if not articles:\n",
    "        print(\"No card divs found on page. Exiting...\")\n",
    "        \n",
    "        \n",
    "    for article in articles:\n",
    "        title_element = article.find('tr', {'class': 'js-filter-all js-filter-sets js-filter-mocs even' })\n",
    "        title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "        print(title)\n",
    "   \n",
    "        # Print or store the extracted information\n",
    "    print(article.get('class'))\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"Failed to fetch the page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d8bee-1c3e-4842-b24e-a308bd6a97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'js-filter-all js-filter-sets js-filter-mocs even'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471ed09-8fbb-45af-b200-aa2944a799b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def obtener_info_idealista(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    respuesta = requests.get(url, headers=headers)\n",
    "\n",
    "    if respuesta.status_code == 200:\n",
    "        soup = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "\n",
    "        # Extracting title\n",
    "        title_element = soup.find('div', {'class': 'ElementLeaf_wrapper__1CJSy'})\n",
    "        title = title_element.get_text() if title_element else \"Title not found\"\n",
    "\n",
    "        return title\n",
    "    else:\n",
    "        print(\"Error al obtener la página:\", respuesta.status_code)\n",
    "        return None\n",
    "\n",
    "url_idealista = \"https://www.lego.com/es-es/pick-and-build/pick-a-brick\"\n",
    "info_title = obtener_info_idealista(url_idealista)\n",
    "\n",
    "if info_title:o_titl\n",
    "    print(\"Título:\", infe)\n",
    "else:\n",
    "    print(\"No se pudo obtener la información.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfbcece-0e8d-4e8d-8374-f1f6778ac45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE PIECES PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc97f27-e7d1-47b3-982e-89f8a47260fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No card divs found on page 150. Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14828"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_data(base_url, start_page):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    page_number = start_page\n",
    "    \n",
    "    information=[]\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{base_url}{page_number}\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all articles with class \"ElementLeaf_wrapper__1CJSy\"\n",
    "            articles = soup.find_all('div', class_='ElementLeaf_wrapper__1CJSy')\n",
    "            \n",
    "            if not articles:\n",
    "                print(f\"No card divs found on page {page_number}. Exiting...\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                                \n",
    "                title_element = article.find('span', {'class': 'ElementLeaf_elementTitle__xda82'})\n",
    "                title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "\n",
    "                # Extracting Serial\n",
    "                serial_element = article.find('span', {'class': 'Text__BaseText-sc-13i1y3k-0 iEGSLL'})\n",
    "                serial = serial_element.get_text().strip() if serial_element else \"Serial not found\"\n",
    "\n",
    "                # Extracting price\n",
    "                price_element = article.find('span', {'class': 'Text__BaseText-sc-13i1y3k-0 kkcaWu'})\n",
    "                price = price_element.get_text().strip() if price_element else \"Price not found\"\n",
    "\n",
    "                # Print or store the extracted information\n",
    "                information += [{\"Name\":title, \"Serial\":serial, \"Price\":price}]\n",
    "\n",
    "            # Move to the next page\n",
    "            page_number += 1\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f\"Error al obtener la página {page_number}:\", response.status_code)\n",
    "            \n",
    "            break\n",
    "    return information\n",
    "            \n",
    "\n",
    "base_url = \"https://www.lego.com/en-us/pick-and-build/pick-a-brick?page=\"\n",
    "start_page_number = 1\n",
    "\n",
    "listaelementos=scrape_data(base_url, start_page_number)\n",
    "len(listaelementos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b38677-d5dd-457f-b4a3-e395db8748d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(listaelementos).to_csv('pickabric_price_new.csv',index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7adbf1d-5e43-4e71-b475-791a079b992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE PAGES LESS 25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee853fae-63ab-450d-9d99-ed449f0655cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No card divs found on page 20. Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_data(base_url, start_page):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    page_number = start_page\n",
    "    \n",
    "    information=[]\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{base_url}{page_number}{config_url}\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all articles with class \"ElementLeaf_wrapper__1CJSy\"\n",
    "            articles = soup.find_all('article', class_='ProductLeaf_wrapper__H0TCb')\n",
    "\n",
    "            \n",
    "            if not articles:\n",
    "                print(f\"No card divs found on page {page_number}. Exiting...\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                \n",
    "                \n",
    "                title_element = article.find('span', {'class': 'markup'})\n",
    "                title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "\n",
    "                # Extracting Pieces\n",
    "                pieces_element = article.find('span', {'data-test': 'product-leaf-piece-count-label'})\n",
    "                pieces = pieces_element.get_text().strip() if pieces_element else \"Serial not found\"\n",
    "               \n",
    "                # Extracting price\n",
    "                price_element = article.find('span', {'class': 'price-sm-bold'})\n",
    "                price = price_element.get_text().strip() if price_element else \"Price not found\"\n",
    "\n",
    "                # Print or store the extracted information\n",
    "                information += [{\"Name\":title, \"Pieces\":pieces, \"Price\":price}]\n",
    "\n",
    "            # Move to the next page\n",
    "            page_number += 1\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f\"Error al obtener la página {page_number}:\", response.status_code)\n",
    "            \n",
    "            break\n",
    "    return information\n",
    "            \n",
    "\n",
    "base_url = \"https://www.lego.com/en-us/categories/price-under-25-dollars?page=\"\n",
    "config_url=\"&filters.i0.key=categories.id&filters.i0.values.i0=12ba8640-7fb5-4281-991d-ac55c65d8001&offset=0\"\n",
    "start_page_number = 1\n",
    "\n",
    "listaelementos2=scrape_data(base_url, start_page_number)\n",
    "len(listaelementos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3704c3-dd72-4502-b8dc-981a1f86b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE LESS 50$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91fb1e6-d031-4345-956d-739886435d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No card divs found on page 16. Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_data(base_url, start_page):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    page_number = start_page\n",
    "    \n",
    "    information=[]\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{base_url}{page_number}{config_url}\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all articles with class \"ElementLeaf_wrapper__1CJSy\"\n",
    "            articles = soup.find_all('article', class_='ProductLeaf_wrapper__H0TCb')\n",
    "\n",
    "            \n",
    "            if not articles:\n",
    "                print(f\"No card divs found on page {page_number}. Exiting...\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                                \n",
    "                title_element = article.find('span', {'class': 'markup'})\n",
    "                title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "\n",
    "                # Extracting Pieces\n",
    "                pieces_element = article.find('span', {'data-test': 'product-leaf-piece-count-label'})\n",
    "                pieces = pieces_element.get_text().strip() if pieces_element else \"Serial not found\"\n",
    "               \n",
    "                # Extracting price\n",
    "                price_element = article.find('span', {'class': 'price-sm-bold'})\n",
    "                price = price_element.get_text().strip() if price_element else \"Price not found\"\n",
    "\n",
    "                # Print or store the extracted information\n",
    "                information += [{\"Name\":title, \"Pieces\":pieces, \"Price\":price}]\n",
    "\n",
    "            # Move to the next page\n",
    "            page_number += 1\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f\"Error al obtener la página {page_number}:\", response.status_code)\n",
    "            \n",
    "            break\n",
    "    return information\n",
    "            \n",
    "\n",
    "base_url = \"https://www.lego.com/en-us/categories/price-25-50-dollars?page=\"\n",
    "config_url=\"&filters.i0.key=categories.id&filters.i0.values.i0=12ba8640-7fb5-4281-991d-ac55c65d8001&offset=0\"\n",
    "start_page_number = 1\n",
    "\n",
    "listaelementos3=scrape_data(base_url, start_page_number)\n",
    "len(listaelementos3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5135f-f2f3-4e25-96ee-5d5a33d84a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE LESS 75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "639f8bd2-1ef1-4702-a45d-e7d562bb35cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No card divs found on page 7. Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_data(base_url, start_page):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    page_number = start_page\n",
    "    \n",
    "    information=[]\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{base_url}{page_number}{config_url}\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all articles with class \"ElementLeaf_wrapper__1CJSy\"\n",
    "            articles = soup.find_all('article', class_='ProductLeaf_wrapper__H0TCb')\n",
    "\n",
    "            \n",
    "            if not articles:\n",
    "                print(f\"No card divs found on page {page_number}. Exiting...\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                                \n",
    "                title_element = article.find('span', {'class': 'markup'})\n",
    "                title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "\n",
    "                # Extracting Pieces\n",
    "                pieces_element = article.find('span', {'data-test': 'product-leaf-piece-count-label'})\n",
    "                pieces = pieces_element.get_text().strip() if pieces_element else \"Serial not found\"\n",
    "               \n",
    "                # Extracting price\n",
    "                price_element = article.find('span', {'class': 'price-sm-bold'})\n",
    "                price = price_element.get_text().strip() if price_element else \"Price not found\"\n",
    "\n",
    "                # Print or store the extracted information\n",
    "                information += [{\"Name\":title, \"Pieces\":pieces, \"Price\":price}]\n",
    "\n",
    "            # Move to the next page\n",
    "            page_number += 1\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f\"Error al obtener la página {page_number}:\", response.status_code)\n",
    "            \n",
    "            break\n",
    "    return information\n",
    "            \n",
    "\n",
    "base_url = \"https://www.lego.com/en-us/categories/price-50-75-dollars?page=\"\n",
    "config_url=\"&filters.i0.key=categories.id&filters.i0.values.i0=12ba8640-7fb5-4281-991d-ac55c65d8001&offset=0\"\n",
    "start_page_number = 1\n",
    "\n",
    "listaelementos4=scrape_data(base_url, start_page_number)\n",
    "len(listaelementos4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10441d62-d45c-4c26-9f58-d906436beb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE LESS 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9b86a4-48b5-474a-8925-cac0b6379944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No card divs found on page 7. Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_data(base_url, start_page):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    page_number = start_page\n",
    "    \n",
    "    information=[]\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{base_url}{page_number}{config_url}\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all articles with class \"ElementLeaf_wrapper__1CJSy\"\n",
    "            articles = soup.find_all('article', class_='ProductLeaf_wrapper__H0TCb')\n",
    "\n",
    "            \n",
    "            if not articles:\n",
    "                print(f\"No card divs found on page {page_number}. Exiting...\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                                \n",
    "                title_element = article.find('span', {'class': 'markup'})\n",
    "                title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "\n",
    "                # Extracting Pieces\n",
    "                pieces_element = article.find('span', {'data-test': 'product-leaf-piece-count-label'})\n",
    "                pieces = pieces_element.get_text().strip() if pieces_element else \"Serial not found\"\n",
    "               \n",
    "                # Extracting price\n",
    "                price_element = article.find('span', {'class': 'price-sm-bold'})\n",
    "                price = price_element.get_text().strip() if price_element else \"Price not found\"\n",
    "\n",
    "                # Print or store the extracted information\n",
    "                information += [{\"Name\":title, \"Pieces\":pieces, \"Price\":price}]\n",
    "\n",
    "            # Move to the next page\n",
    "            page_number += 1\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f\"Error al obtener la página {page_number}:\", response.status_code)\n",
    "            \n",
    "            break\n",
    "    return information\n",
    "            \n",
    "\n",
    "base_url = \"https://www.lego.com/en-us/categories/price-75-100-dollars?page=\"\n",
    "config_url=\"&filters.i0.key=categories.id&filters.i0.values.i0=12ba8640-7fb5-4281-991d-ac55c65d8001&offset=0\"\n",
    "start_page_number = 1\n",
    "\n",
    "listaelementos5=scrape_data(base_url, start_page_number)\n",
    "len(listaelementos5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cee2b6-402c-412b-99cc-1d11d51032cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE MORE 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f656d479-2c68-4820-a3b3-cfdc48feb4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No card divs found on page 11. Exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_data(base_url, start_page):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    page_number = start_page\n",
    "    \n",
    "    information=[]\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        url = f\"{base_url}{page_number}{config_url}\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all articles with class \"ElementLeaf_wrapper__1CJSy\"\n",
    "            articles = soup.find_all('article', class_='ProductLeaf_wrapper__H0TCb')\n",
    "\n",
    "            \n",
    "            if not articles:\n",
    "                print(f\"No card divs found on page {page_number}. Exiting...\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                                \n",
    "                title_element = article.find('span', {'class': 'markup'})\n",
    "                title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "\n",
    "                # Extracting Pieces\n",
    "                pieces_element = article.find('span', {'data-test': 'product-leaf-piece-count-label'})\n",
    "                pieces = pieces_element.get_text().strip() if pieces_element else \"Serial not found\"\n",
    "               \n",
    "                # Extracting price\n",
    "                price_element = article.find('span', {'class': 'price-sm-bold'})\n",
    "                price = price_element.get_text().strip() if price_element else \"Price not found\"\n",
    "\n",
    "                # Print or store the extracted information\n",
    "                information += [{\"Name\":title, \"Pieces\":pieces, \"Price\":price}]\n",
    "\n",
    "            # Move to the next page\n",
    "            page_number += 1\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f\"Error al obtener la página {page_number}:\", response.status_code)\n",
    "            \n",
    "            break\n",
    "    return information\n",
    "            \n",
    "\n",
    "base_url = \"https://www.lego.com/en-us/categories/price-over-100-dollars?page=\"\n",
    "config_url=\"&filters.i0.key=categories.id&filters.i0.values.i0=12ba8640-7fb5-4281-991d-ac55c65d8001&offset=0\"\n",
    "start_page_number = 1\n",
    "\n",
    "listaelementos6=scrape_data(base_url, start_page_number)\n",
    "len(listaelementos6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f05442-4e1e-4a76-bd65-f2ae71bdf298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Pieces</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iron Spider-Man</td>\n",
       "      <td>91</td>\n",
       "      <td>$9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stitch</td>\n",
       "      <td>152</td>\n",
       "      <td>$9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potted Groot</td>\n",
       "      <td>113</td>\n",
       "      <td>$9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spring Festival Mickey Mouse</td>\n",
       "      <td>120</td>\n",
       "      <td>$9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cherry Blossoms</td>\n",
       "      <td>430</td>\n",
       "      <td>$14.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Coding Express</td>\n",
       "      <td>234</td>\n",
       "      <td>$249.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Bookshop</td>\n",
       "      <td>2504</td>\n",
       "      <td>$199.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Chinese New Year Temple Fair</td>\n",
       "      <td>1664</td>\n",
       "      <td>$119.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>Black Panther</td>\n",
       "      <td>2961</td>\n",
       "      <td>$349.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Table Football</td>\n",
       "      <td>2339</td>\n",
       "      <td>$249.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>957 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name Pieces    Price\n",
       "0                 Iron Spider-Man     91    $9.99\n",
       "1                          Stitch    152    $9.99\n",
       "2                    Potted Groot    113    $9.99\n",
       "3    Spring Festival Mickey Mouse    120    $9.99\n",
       "4                 Cherry Blossoms    430   $14.99\n",
       "..                            ...    ...      ...\n",
       "952                Coding Express    234  $249.95\n",
       "953                      Bookshop   2504  $199.99\n",
       "954  Chinese New Year Temple Fair   1664  $119.99\n",
       "955                 Black Panther   2961  $349.99\n",
       "956                Table Football   2339  $249.99\n",
       "\n",
       "[957 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_catalog=listaelementos2 + listaelementos3 + listaelementos4 + listaelementos5 + listaelementos6\n",
    "pd.DataFrame(web_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2975f9-c5b4-4371-86cc-03afbbc065fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(web_catalog).to_csv('web_catalog_price.csv',index=False,sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
